# Performance Analysis & Optimization

## üìä Current System Analysis

### 1. OpenAI API Calls Per Request

**Current Flow:**
```
User Query ‚Üí Workflow ‚Üí Retrieval ‚Üí Answer Generation ‚Üí Response
                            ‚Üì              ‚Üì
                    Elasticsearch    OpenAI API (1 call)
```

**Answer:** **1 OpenAI API call per request**

**Location:** `backend/app/services/generation/answer_generator.py:115-123`

```python
response = self.client.chat.completions.create(
    model=settings.ANSWER_GENERATION_MODEL,  # gpt-3.5-turbo
    messages=messages,
    temperature=settings.ANSWER_TEMPERATURE,
    max_tokens=settings.ANSWER_MAX_TOKENS,
    ...
)
```

---

### 2. Current Response Time Breakdown

**Estimated Time per Request (Single Worker):**

| Step | Time | Percentage | Details |
|------|------|------------|---------|
| **1. Input Validation** | ~10ms | 0.3% | Minimal processing |
| **2. Elasticsearch Retrieval** | ~200-500ms | 10-15% | Hybrid search (BM25 + Vector) |
| **3. Reranking** | ~300-800ms | 15-25% | Cross-encoder model inference |
| **4. OpenAI API Call** | ~2000-4000ms | 60-70% | **BOTTLENECK** |
| **5. Output Formatting** | ~10ms | 0.3% | Minimal processing |
| **Total** | **~2.5-5.5s** | 100% | Average: ~3-4s |

**Key Findings:**
- ‚úÖ **OpenAI API is the main bottleneck** (60-70% of total time)
- ‚úÖ Reranking is second bottleneck (15-25%)
- ‚úÖ Elasticsearch retrieval is fast (10-15%)
- ‚úÖ Other steps are negligible (<1%)

---

### 3. Current Model: GPT-3.5-Turbo

**Specifications:**
- **Model:** `gpt-3.5-turbo`
- **Max Tokens:** 500 (output)
- **Temperature:** 0.7
- **Average Latency:** 2-4 seconds
- **Cost:** $0.0015 / 1K input tokens, $0.002 / 1K output tokens

**Typical Token Usage per Request:**
- Input tokens: ~800-1200 (system prompt + context + query)
- Output tokens: ~200-400 (answer)
- Total: ~1000-1600 tokens
- **Cost per request:** ~$0.002-0.003 (0.2-0.3 cents)

---

## üöÄ Optimization Strategies

### Strategy 1: Switch to GPT-gpt-5-nano-2025-08-07 (Recommended)

**Model:** `gpt-gpt-5-nano-2025-08-07` (latest small model from OpenAI)

**Benefits:**
- ‚úÖ **50% faster** than GPT-3.5-turbo (1-2s vs 2-4s)
- ‚úÖ **60% cheaper** ($0.00015 / 1K input, $0.0006 / 1K output)
- ‚úÖ **Better quality** (GPT-4 level reasoning)
- ‚úÖ **Larger context window** (128K tokens vs 16K)
- ‚úÖ **Better multilingual support** (Vietnamese)

**Expected Improvement:**
- Response time: **3-4s ‚Üí 1.5-2.5s** (40-50% faster)
- Cost: **$0.002-0.003 ‚Üí $0.0008-0.0012** (60% cheaper)

---

### Strategy 2: Parallel Processing (Already Implemented)

**Current Implementation:**
- ‚úÖ Singleton pattern (models loaded once)
- ‚úÖ Multi-worker support (via `run.py`)
- ‚úÖ Thread-safe database connections

**Benefits:**
- ‚úÖ Multiple requests handled concurrently
- ‚úÖ No model reloading overhead
- ‚úÖ Better resource utilization

**Expected Throughput:**
- Single worker: ~15-20 req/min
- 4 workers: ~60-80 req/min
- 8 workers: ~120-160 req/min

---

### Strategy 3: Caching (Future Optimization)

**Not yet implemented, but recommended:**

1. **Query Caching:**
   - Cache identical/similar queries
   - Use Redis for fast lookup
   - Expected hit rate: 20-30%
   - Speedup for cached queries: **3-4s ‚Üí 50ms** (60-80x faster)

2. **Embedding Caching:**
   - Cache query embeddings
   - Reduce embedding computation time
   - Speedup: ~100-200ms saved per request

3. **Context Caching:**
   - Cache retrieved contexts for similar queries
   - Reduce Elasticsearch + reranking time
   - Speedup: ~500-1000ms saved per request

---

### Strategy 4: Streaming Responses (Future Optimization)

**Not yet implemented:**

```python
# Stream tokens as they're generated
response = self.client.chat.completions.create(
    model="gpt-gpt-5-nano-2025-08-07",
    messages=messages,
    stream=True  # Enable streaming
)

for chunk in response:
    yield chunk.choices[0].delta.content
```

**Benefits:**
- ‚úÖ **Perceived latency reduced** (user sees response immediately)
- ‚úÖ Better UX (progressive loading)
- ‚úÖ Same total time, but feels faster

---

### Strategy 5: Reduce Reranking Overhead

**Current:**
- Retrieve 20 candidates ‚Üí Rerank ‚Üí Return top 5
- Reranking time: ~300-800ms

**Optimization Options:**

1. **Reduce candidates:**
   - Retrieve 10 candidates instead of 20
   - Speedup: ~150-400ms saved
   - Trade-off: Slightly lower quality

2. **Disable reranking for simple queries:**
   - Use heuristics to detect simple queries
   - Skip reranking for those
   - Speedup: ~300-800ms saved for simple queries

3. **Use faster reranker model:**
   - Current: `cross-encoder/ms-marco-MiniLM-L-12-v2`
   - Alternative: `cross-encoder/ms-marco-TinyBERT-L-2-v2` (2x faster)
   - Trade-off: Slightly lower quality

---

## üìà Recommended Implementation Plan

### Phase 1: Switch to GPT-gpt-5-nano-2025-08-07 (Immediate)

**Impact:** 40-50% faster, 60% cheaper, better quality

**Steps:**
1. Update `.env`: `ANSWER_GENERATION_MODEL=gpt-gpt-5-nano-2025-08-07`
2. Update `config.py`: Change default model
3. Test and verify quality
4. Deploy

**Estimated Time:** 5 minutes
**Expected Result:** 3-4s ‚Üí 1.5-2.5s per request

---

### Phase 2: Optimize Reranking (Short-term)

**Impact:** 10-20% faster

**Steps:**
1. Reduce candidates from 20 to 15
2. Test quality impact
3. Consider faster reranker model

**Estimated Time:** 30 minutes
**Expected Result:** 1.5-2.5s ‚Üí 1.3-2.2s per request

---

### Phase 3: Implement Caching (Medium-term)

**Impact:** 60-80x faster for cached queries (20-30% hit rate)

**Steps:**
1. Add Redis dependency
2. Implement query caching
3. Implement embedding caching
4. Monitor cache hit rate

**Estimated Time:** 2-4 hours
**Expected Result:** 
- Cached: 1.3-2.2s ‚Üí 50-100ms
- Uncached: 1.3-2.2s (no change)
- Overall: ~20-30% faster (weighted average)

---

### Phase 4: Streaming Responses (Long-term)

**Impact:** Better UX, perceived latency reduced

**Steps:**
1. Implement streaming in answer generator
2. Update API to support SSE (Server-Sent Events)
3. Update frontend to handle streaming

**Estimated Time:** 4-8 hours
**Expected Result:** Same total time, but user sees response immediately

---

## üéØ Summary

### Current Performance
- **Response Time:** 3-4 seconds per request
- **OpenAI Calls:** 1 per request
- **Bottleneck:** OpenAI API (60-70% of time)
- **Model:** GPT-3.5-turbo
- **Cost:** $0.002-0.003 per request

### After Phase 1 (GPT-gpt-5-nano-2025-08-07)
- **Response Time:** 1.5-2.5 seconds per request (**40-50% faster**)
- **Cost:** $0.0008-0.0012 per request (**60% cheaper**)
- **Quality:** Better (GPT-4 level)

### After All Phases
- **Response Time:** 
  - Cached: 50-100ms (**60-80x faster**)
  - Uncached: 1.3-2.2s (**50-60% faster**)
  - Overall: ~1-2s average (**60-70% faster**)
- **Cost:** 60% cheaper
- **UX:** Much better (streaming)

---

## üîß Implementation Commands

### Quick Start: Switch to GPT-gpt-5-nano-2025-08-07

```bash
# 1. Update .env file
echo "ANSWER_GENERATION_MODEL=gpt-gpt-5-nano-2025-08-07" >> backend/.env

# 2. Restart server
python run.py --mode single

# 3. Test
python backend/scripts/test_api.py
```

### Test Performance

```bash
# Test concurrent requests
python backend/scripts/test_concurrent.py
```

---

## üìù Notes

1. **GPT-gpt-5-nano-2025-08-07 is the best immediate optimization** (minimal effort, maximum impact)
2. **Multi-worker setup is already optimal** (singleton pattern implemented)
3. **Caching would provide the biggest long-term improvement** (but requires more work)
4. **Streaming would improve UX significantly** (but doesn't reduce total time)


